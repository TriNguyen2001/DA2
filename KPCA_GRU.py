# #-------------------------------------------------------Preprocessing--------------------------------------------
# import pandas as pd
# import numpy as np
# import seaborn as sns
# import matplotlib.pyplot as plt
# import os

# from sklearn.preprocessing import MinMaxScaler
# from sklearn.feature_selection import SelectKBest, f_regression
# from sklearn.decomposition import KernelPCA

# # ------------------------
# # 1. ƒê·ªçc , xu·∫•t d·ªØ li·ªáu
# data = pd.read_excel(r"D:\TriNguyen\242\DA2\visual code\Data\example_solar_forecast_input.xlsx")
# output_dir = r"D:\TriNguyen\242\DA2\visual code\Data"
# os.makedirs(output_dir, exist_ok=True)  # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥

# # 2. Lo·∫°i b·ªè c·ªôt kh√¥ng ph·∫£i s·ªë (nh∆∞ datetime ho·∫∑c object)
# data = data.select_dtypes(include=["number"])

# # 3. X·ª≠ l√Ω gi√° tr·ªã thi·∫øu
# data = data.fillna(data.mean())

# # 4. ƒê·∫∑t X v√† y
# y = data["power"]
# X = data.drop(columns=["power"])

# # 5. Chu·∫©n h√≥a d·ªØ li·ªáu
# scaler = MinMaxScaler()
# X_scaled = scaler.fit_transform(X)
# X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)

# # ------------------------
# # üìå 1. Heatmap Pearson to√†n b·ªô
# cols_to_exclude = ['hour', 'day', 'month']
# heatmap_data = data.drop(columns=[col for col in cols_to_exclude if col in data.columns])

# plt.figure(figsize=(12, 8))
# sns.heatmap(heatmap_data.corr(), annot=True, cmap="coolwarm")
# plt.title("Heatmap Pearson Correlation Matrix")
# plt.tight_layout()
# plt.show()

# # ------------------------
# # üìå 2. Heatmap gi·ªØa power hi·ªán t·∫°i v√† qu√° kh·ª© (t-1 ƒë·∫øn t-24)
# for lag in range(1, 25):
#     data[f'power_t-{lag}'] = data['power'].shift(lag)

# data_lagged = data.dropna()
# power_lags = [f'power_t-{i}' for i in range(1, 25)]
# corr_lag = data_lagged[power_lags + ['power']].corr()

# plt.figure(figsize=(14, 5))
# sns.heatmap(corr_lag[['power']].T, annot=True, cmap="YlGnBu")
# plt.title("T∆∞∆°ng quan gi·ªØa c√¥ng su·∫•t hi·ªán t·∫°i v√† qu√° kh·ª© (t-1 ƒë·∫øn t-24)")
# plt.xlabel("C√¥ng su·∫•t hi·ªán t·∫°i (power)")
# plt.ylabel("L√πi th·ªùi gian")
# plt.tight_layout()
# plt.show()

# # T·∫°o danh s√°ch c√°c c·ªôt tr·ªÖ
# lag_features = [f'power_t-{i}' for i in range(1, 25)]

# # T·∫°o DataFrame so s√°nh gi·ªØa power hi·ªán t·∫°i v√† qu√° kh·ª©
# comparison_df = data_lagged[lag_features + ['power']].copy()

# # Xu·∫•t ra file Excel
# output_path = os.path.join(output_dir, "power_comparison.xlsx")
# comparison_df.to_excel(output_path, index=False)

# print("‚úÖ ƒê√£ l∆∞u file power__comparison.xlsx ƒë·ªÉ so s√°nh!")


# # ------------------------
# # üìå 3. F-score - Feature Importance
# X_filtered = X_scaled_df.drop(columns=[col for col in ['hour', 'day', 'month'] if col in X_scaled_df.columns])
# f_score_selector = SelectKBest(score_func=f_regression, k='all')
# f_score_selector.fit(X_filtered, y)
# f_scores = f_score_selector.scores_

# plt.figure(figsize=(12, 5))
# sns.barplot(x=X_filtered.columns, y=f_scores, palette='viridis')
# plt.title("M·ª©c ƒë·ªô quan tr·ªçng c·ªßa ƒë·∫∑c tr∆∞ng (F-score) - lo·∫°i b·ªè hour/day/month")
# plt.ylabel("F-score")
# plt.xticks(rotation=45)
# plt.tight_layout()
# plt.show()

# # ------------------------
# # üìå 4. Gi·∫£m chi·ªÅu b·∫±ng Kernel PCA
# kpca = KernelPCA(n_components=2, kernel='rbf', gamma=0.1)
# X_kpca = kpca.fit_transform(X_scaled)

# explained_variance = np.var(X_kpca, axis=0)
# explained_ratio = explained_variance / np.sum(explained_variance)

# print("üìä ƒê√≥ng g√≥p ph∆∞∆°ng sai c·ªßa t·ª´ng th√†nh ph·∫ßn KPCA:", explained_ratio)

# plt.figure(figsize=(6, 4))
# plt.bar(['PC1', 'PC2'], explained_ratio * 100, color='steelblue')
# plt.title("ƒê√≥ng g√≥p ph∆∞∆°ng sai c√°c th√†nh ph·∫ßn KPCA")
# plt.ylabel("T·ª∑ l·ªá (%)")
# plt.grid(True, linestyle='--', alpha=0.5)
# plt.tight_layout()
# plt.show()


# # ------------------------------------------------ TRAINING GRU------------------------------------------------
# # üì¶ T·∫°o X_final t·ª´ lag features v√† KPCA


# # B∆∞·ªõc 1: Th√™m l·∫°i power lag features v√¨ tr∆∞·ªõc ƒë√≥ ƒë√£ dropna r·ªìi
# for lag in range(1, 25):
#     data[f'power_t-{lag}'] = data['power'].shift(lag)

# # B∆∞·ªõc 2: T·∫°o l·∫°i data_lagged sau khi c√≥ KPCA
# data_lagged = data.dropna().reset_index(drop=True)

# # B∆∞·ªõc 3: T·∫°o DataFrame t·ª´ KPCA
# kpca_df = pd.DataFrame(X_kpca, columns=["PC1", "PC2"])
# kpca_df = kpca_df.iloc[-len(data_lagged):].reset_index(drop=True)  # kh·ªõp chi·ªÅu

# # B∆∞·ªõc 4: K·∫øt h·ª£p th√†nh X_final
# lag_features = [f'power_t-{i}' for i in range(1, 25)]
# X_final = pd.concat([
#     data_lagged[lag_features].reset_index(drop=True),
#     kpca_df
# ], axis=1)

# # B∆∞·ªõc 5: T·∫°o y_final
# y_final = data_lagged["power"].reset_index(drop=True)

# # B∆∞·ªõc 6: Xu·∫•t ra file Excel ho·∫∑c CSV
# X_final["target_power"] = y_final
# output_path_for_GRU = os.path.join(output_dir, "X_final_for_GRU.xlsx")
# X_final.to_excel(output_path_for_GRU, index=False)
# print("‚úÖ Xu·∫•t file X_final_for_GRU.xlsx th√†nh c√¥ng!")








# #---------------------------------------------------new-----------------------------------------------------------


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import GRU, Dense
from sklearn.preprocessing import MinMaxScaler

# === 1. ƒê·ªåC FILE L·ªäCH S·ª¨ ===
history_df = pd.read_excel(r"D:\TriNguyen\242\DA2\visual code\Data\History_power_and_rad.xlsx")
history_df['Datetime'] = pd.to_datetime(history_df['Datetime'])
history_df.set_index('Datetime', inplace=True)
history_df = history_df.dropna()

# === 2. TI·ªÄN X·ª¨ L√ù + SCALE ===
features = ['Temp', 'Hum', 'Wind', 'Rad']
target = 'Power'

scaler_X = MinMaxScaler()
scaler_y = MinMaxScaler()

X_scaled = scaler_X.fit_transform(history_df[features])
y_scaled = scaler_y.fit_transform(history_df[[target]])

# === 3. T·∫†O D·ªÆ LI·ªÜU THEO SEQ (v√≠ d·ª•: 6 b∆∞·ªõc qu√° kh·ª©) ===
def create_sequences(X, y, time_steps=6):
    Xs, ys = [], []
    for i in range(time_steps, len(X)):
        Xs.append(X[i-time_steps:i])
        ys.append(y[i])
    return np.array(Xs), np.array(ys)

X_seq, y_seq = create_sequences(X_scaled, y_scaled)

# === 4. T·∫†O V√Ä HU·∫§N LUY·ªÜN M√î H√åNH GRU ===
model = Sequential([
    GRU(64, input_shape=(X_seq.shape[1], X_seq.shape[2]), return_sequences=False),
    Dense(1)
])
model.compile(loss='mse', optimizer='adam')
model.fit(X_seq, y_seq, epochs=20, batch_size=32, verbose=1)




# === 5. ƒê·ªåC FILE D·ª∞ B√ÅO TH·ªúI TI·∫æT ===
nwp_df = pd.read_excel(r"D:\TriNguyen\242\DA2\visual code\Data\NWP.xlsx")
nwp_df['Datetime'] = pd.to_datetime(nwp_df['Datetime'])
nwp_df.set_index('Datetime', inplace=True)
nwp_df = nwp_df.dropna()

# === 6. CHU·∫®N H√ìA INPUT D·ª∞ B√ÅO ===
nwp_scaled = scaler_X.transform(nwp_df[features])

# T·∫°o chu·ªói input (d√πng ch√≠nh data d·ª± b√°o 6 b∆∞·ªõc g·∫ßn nh·∫•t)
time_steps = 6
X_forecast = []

for i in range(time_steps, len(nwp_scaled)):
    X_forecast.append(nwp_scaled[i-time_steps:i])

X_forecast = np.array(X_forecast)

# === 7. D·ª∞ B√ÅO V√Ä INVERSE SCALE ===
y_pred_scaled = model.predict(X_forecast)
y_pred = scaler_y.inverse_transform(y_pred_scaled)

# === 8. V·∫º BI·ªÇU ƒê·ªí ===
forecast_time = nwp_df.index[time_steps:]

plt.figure(figsize=(12, 5))
plt.plot(forecast_time, y_pred, label="D·ª± ƒëo√°n c√¥ng su·∫•t GRU", color='orange')
plt.title("D·ª± b√°o c√¥ng su·∫•t t·ª´ d·ªØ li·ªáu d·ª± b√°o th·ªùi ti·∫øt (GRU)")
plt.xlabel("Th·ªùi gian")
plt.ylabel("C√¥ng su·∫•t (W)")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()
